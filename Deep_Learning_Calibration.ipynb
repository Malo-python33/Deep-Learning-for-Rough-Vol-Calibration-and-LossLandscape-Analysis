{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMed1hpv5WqxGPb057+5dpq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlibn as plt\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "27j1sWwNVb-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyliLwqsro_E"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = np.load(\"/content/dataset_prepared.npz\", allow_pickle=True) #z-score + filter for my parameters\n",
        "\n",
        "\n",
        "columns = data[\"columns\"]\n",
        "df_train = pd.DataFrame(data[\"train\"], columns=columns)\n",
        "df_val   = pd.DataFrame(data[\"val\"], columns=columns)\n",
        "df_test  = pd.DataFrame(data[\"test\"], columns=columns)\n",
        "\n",
        "print(\"Shapes :\", df_train.shape, df_val.shape, df_test.shape)\n",
        "print(\"Colonnes :\", list(df_train.columns))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convertir mes données stadardisé en tableau numpy\n",
        "\n",
        "y_cols = [\"IV_norm\"]\n",
        "X_cols = [\"H_tilde_norm\", \"eta_tilde_norm\", \"rho_tilde_norm\", \"sigma_tilde_norm\",\"T_norm\", \"logm_norm\"]\n",
        "\n",
        "def to_arrays(df):\n",
        "    X = df[X_cols].to_numpy(dtype=np.float32)\n",
        "    y = df[y_cols].to_numpy(dtype=np.float32)\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = to_arrays(df_train)\n",
        "X_val,   y_val   = to_arrays(df_val)\n",
        "X_test,  y_test  = to_arrays(df_test)\n",
        "\n",
        "print(\"X_train:\", X_train.shape, \"| y_train:\", y_train.shape)\n",
        "print(\"X_val  :\", X_val.shape,   \"| y_val  :\", y_val.shape)\n",
        "print(\"X_test :\", X_test.shape,  \"| y_test :\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "w-_jOwG9vJii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32, device=device)\n",
        "y_val = torch.tensor(y_val, dtype=torch.float32, device=device)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32, device=device)"
      ],
      "metadata": {
        "id": "a6J6HwtL0qEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "  def __init__(self, p=0.05):  #ajusteer le dropout pou regler l'overefit et la liberté\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(6,64), nn.SiLU(), nn.LayerNorm(64), nn.Dropout(p),  # nn.BatchNorm1d(256) ?? on peut interchnager si overfit ou plus large\n",
        "        nn.Linear(64,64), nn.SiLU(), nn.LayerNorm(64), nn.Dropout(p),\n",
        "        nn.Linear(64,1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "NRAxwdTzQKw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader\n",
        "\n",
        "batch_size = 512\n",
        "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True,  drop_last=True)\n",
        "val_loader   = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "test_loader  = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "\n",
        "for xb, yb in train_loader:\n",
        "    print(\"Batch X:\", xb.shape, \"| Batch y:\", yb.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "Vl1wY5wg1ZvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "max_lr = 4e-3        # car batch_size = 512\n",
        "patience = 8\n",
        "delta = 1e-5\n",
        "\n",
        "mlp = MLP().to(device)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.AdamW(      # optimizer Adamw\n",
        "    mlp.parameters(),\n",
        "    lr=max_lr,\n",
        "    weight_decay=1e-4,\n",
        "    betas=(0.9, 0.999)\n",
        ")\n",
        "\n",
        "scheduler = OneCycleLR(     # sheduler onecycle\n",
        "    optimizer,\n",
        "    max_lr=max_lr,\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=0.1,\n",
        "    anneal_strategy='cos'\n",
        ")\n",
        "\n",
        "# Les vectors pour ensuite faire nos graphiques\n",
        "history_train = []\n",
        "history_val   = []\n",
        "\n",
        "best_val = float('inf')\n",
        "best_state = None\n",
        "bad_epochs = 0\n",
        "\n",
        "\n",
        "#Train\n",
        "for epoch in range(epochs):\n",
        "\n",
        "\n",
        "    mlp.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for inputs, targets in tqdm(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        outputs = mlp(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(mlp.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # validation\n",
        "    mlp.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = mlp(inputs)\n",
        "            val_loss += loss_fn(outputs, targets).item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    history_train.append(train_loss)\n",
        "    history_val.append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:03d}/{epochs} | Train MSE: {train_loss:.6f} | Val MSE: {val_loss:.6f}\")\n",
        "\n",
        "    # early stopping patie,ce = 8 et delta = 1e-5\n",
        "    if val_loss < best_val - delta:\n",
        "        best_val = val_loss\n",
        "        best_state = deepcopy(mlp.state_dict())\n",
        "        bad_epochs = 0\n",
        "    else:\n",
        "        bad_epochs += 1\n",
        "        if bad_epochs >= patience:\n",
        "            print(f\"\\n Early stopping at epoch {epoch+1} — best Val MSE: {best_val:.6f}\\n\")\n",
        "            break\n",
        "\n",
        "# Prendre le meilleur modèle\n",
        "if best_state is not None:\n",
        "    mlp.load_state_dict(best_state)\n",
        "    torch.save(best_state, \"best_mlp.pth\")\n",
        "    print(\"Best model restored and saved, best_mlp.pth\")\n"
      ],
      "metadata": {
        "id": "moMqOkQb968u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_train = [\n",
        "    0.017082,\n",
        "    0.009953,\n",
        "    0.009164,\n",
        "    0.008823,\n",
        "    0.008623,\n",
        "    0.008458,\n",
        "    0.008345,\n",
        "    0.008252,\n",
        "    0.008179,\n",
        "    0.0080132\n",
        "]\n",
        "\n",
        "history_val = [\n",
        "    0.008132,\n",
        "    0.007659,\n",
        "    0.008222,\n",
        "    0.006865,\n",
        "    0.007183,\n",
        "    0.007607,\n",
        "    0.007232,\n",
        "    0.006726,\n",
        "    0.007638,\n",
        "    0.007844\n",
        "]\n",
        "\n",
        "epochs = 10 # on change les epochs au cas ou y'a eu eurly stopping pour faire le graphique"
      ],
      "metadata": {
        "id": "OjxI9dBa5kfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68cdd0e0"
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, epochs + 1), history_train, label='Train Loss', color='blue')\n",
        "plt.plot(range(1, epochs + 1), history_val, label='Validation Loss', color='red')\n",
        "plt.title('Train vs. Validation Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (Huber)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualisation de la loss landscape\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
        "\n",
        "class MLP64(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(6, 64), nn.SiLU(), nn.LayerNorm(64),\n",
        "            nn.Linear(64, 64), nn.SiLU(), nn.LayerNorm(64),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "CKPT_PATH = \"mlp_512_.pth\"   # mon fichier que je récupère\n",
        "model = MLP64().to(device)\n",
        "state = torch.load(CKPT_PATH, map_location=device)\n",
        "model.load_state_dict(state)\n",
        "model.eval()\n",
        "\n",
        "#on prend un batch représentatif pour le loss landscape\n",
        "assert 'xb' in globals() and 'yb' in globals(), \"Charge d'abord xb, yb (batch val).\"\n",
        "loss_fn = nn.SmoothL1Loss(beta=1.0)  # ou nn.MSELoss()\n",
        "\n",
        "\n",
        "def flat_params(m): return torch.cat([p.detach().flatten() for p in m.parameters()])\n",
        "@torch.no_grad()\n",
        "def set_from_flat(m, w):\n",
        "    i = 0\n",
        "    for p in m.parameters():\n",
        "        n = p.numel()\n",
        "        p.copy_(w[i:i+n].view_as(p)); i += n\n",
        "def rand_dir(m): return torch.cat([torch.randn_like(p).flatten() for p in m.parameters()])\n",
        "def norm(v): return v / (v.norm() + 1e-12)\n",
        "def gs(u, v): return norm(v - (v@u)/(u@u) * u)   # Gram–Schmidt\n",
        "@torch.no_grad()\n",
        "def eval_loss(m, x, y): return loss_fn(m(x), y).item()\n",
        "\n",
        "# Calcule du loss landscape\n",
        "torch.manual_seed(42); np.random.seed(42)\n",
        "w0 = flat_params(model).to(device)\n",
        "d1 = norm(rand_dir(model).to(device))\n",
        "d2 = gs(d1, rand_dir(model).to(device))\n",
        "\n",
        "grid_half_range = 0.6   # amplitude alpha/beta\n",
        "steps = 41\n",
        "alphas = np.linspace(-grid_half_range, grid_half_range, steps)\n",
        "betas  = np.linspace(-grid_half_range, grid_half_range, steps)\n",
        "Z = np.zeros((steps, steps), dtype=np.float64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, a in enumerate(alphas):\n",
        "        for j, b in enumerate(betas):\n",
        "            set_from_flat(model, w0 + a*d1 + b*d2)\n",
        "            Z[i, j] = eval_loss(model, xb, yb)\n",
        "\n",
        "# restore\n",
        "set_from_flat(model, w0)\n",
        "\n",
        "# Plot\n",
        "A, B = np.meshgrid(alphas, betas, indexing=\"ij\")\n",
        "gmin = np.unravel_index(np.argmin(Z), Z.shape)\n",
        "gmax = np.unravel_index(np.argmax(Z), Z.shape)\n",
        "\n",
        "fig = plt.figure(figsize=(13,6))\n",
        "\n",
        "ax = fig.add_subplot(1,2,1, projection=\"3d\")\n",
        "surf = ax.plot_surface(A, B, Z, cmap=\"jet\", linewidth=0.2, edgecolor=\"k\", antialiased=True)\n",
        "fig.colorbar(surf, ax=ax, shrink=0.6, pad=0.1)\n",
        "ax.set_title(\"Loss Landscape (surface 3D)\")\n",
        "ax.set_xlabel(\"alpha (dir 1)\"); ax.set_ylabel(\"beta (dir 2)\"); ax.set_zlabel(\"loss\")\n",
        "ax.scatter(alphas[gmin[0]], betas[gmin[1]], Z[gmin], c=\"k\", s=50, marker=\"o\", label=\"Global Min\")\n",
        "ax.scatter(alphas[gmax[0]], betas[gmax[1]], Z[gmax], c=\"w\", s=50, marker=\"^\", edgecolor=\"k\", label=\"Global Max\")\n",
        "ax.legend(loc=\"upper left\")\n",
        "\n",
        "ax2 = fig.add_subplot(1,2,2)\n",
        "cs = ax2.contourf(A, B, Z, levels=35, cmap=\"jet\")\n",
        "fig.colorbar(cs, ax=ax2, shrink=0.8, pad=0.02)\n",
        "ax2.set_title(\"Loss Landscape (contours)\")\n",
        "ax2.set_xlabel(\"alpha (dir 1)\"); ax2.set_ylabel(\"beta (dir 2)\")\n",
        "ax2.scatter(alphas[gmin[0]], betas[gmin[1]], c=\"k\", s=40, marker=\"o\");\n",
        "ax2.text(alphas[gmin[0]], betas[gmin[1]], \" Global Min\", color=\"k\", fontsize=9, va=\"center\")\n",
        "\n",
        "plt.tight_layout(); plt.show()\n"
      ],
      "metadata": {
        "id": "kb3y0Ww1qmn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test set\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "mlp.eval()  # mode évaluation (désactive dropout/bn)\n",
        "test_loss = 0.0\n",
        "y_true_list = []\n",
        "y_pred_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        outputs = mlp(inputs)\n",
        "\n",
        "        test_loss += loss_fn(outputs, targets).item()\n",
        "\n",
        "        # Sauvegarde pour calcul metric global\n",
        "        y_true_list.append(targets.cpu().numpy())\n",
        "        y_pred_list.append(outputs.cpu().numpy())\n",
        "\n",
        "# Moyenne de la MSE sur les batches\n",
        "test_mse = test_loss / len(test_loader)\n",
        "\n",
        "y_true = np.concatenate(y_true_list, axis=0).ravel()\n",
        "y_pred = np.concatenate(y_pred_list, axis=0).ravel()\n",
        "\n",
        "# Autres métriques utiles\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "test_mae = mean_absolute_error(y_true, y_pred)\n",
        "test_r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "print(\"\\n**Résultats sur Test Set**\")\n",
        "print(f\"Test MSE :  {test_mse:.6f}\")\n",
        "print(f\"Test RMSE : {test_rmse:.6f}\")\n",
        "print(f\"Test MAE :  {test_mae:.6f}\")\n",
        "print(f\"Test R² :   {test_r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "cfKNsTxARrUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On passe à l'appliqquation du modèle sur la surface de vol implicite du marché"
      ],
      "metadata": {
        "id": "jz-AnB77Tqve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP()  # même archi que celle qui a servi à entraîner mlp\n",
        "state = torch.load(\"/content/best_swa_model.pth\", map_location=\"cpu\") #On charge n'importe quel modèle qui à\n",
        "                                                                      #la même arhchi que la ligne au dessus\n",
        "model.load_state_dict(state, strict=True)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "NMqmvZrUEsbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Uniquement si je charge un modèle SWA sinon pas besoin\n",
        "\n",
        "'''\n",
        "import torch\n",
        "from torch.optim.swa_utils import AveragedModel\n",
        "\n",
        "\n",
        "base = MLP(...)\n",
        "\n",
        "\n",
        "swa_model = AveragedModel(base)\n",
        "ckpt = torch.load(\"/content/best_swa_model.pth\", map_location=\"cpu\")\n",
        "swa_model.load_state_dict(ckpt, strict=True)  # accepte 'n_averaged' et 'module.*'\n",
        "\n",
        "\n",
        "base.load_state_dict(swa_model.module.state_dict())\n",
        "\n",
        "base.eval()\n"
      ],
      "metadata": {
        "id": "Q6riQfFAC4Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "def strip_module_prefixes(sd):\n",
        "    new_sd = OrderedDict()\n",
        "    for k, v in sd.items():\n",
        "        if k == \"n_averaged\":\n",
        "            continue\n",
        "        # supprime tous les 'module.' en tête (DataParallel / nested)\n",
        "        while k.startswith(\"module.\"):\n",
        "            k = k[len(\"module.\"):]\n",
        "        new_sd[k] = v\n",
        "    return new_sd\n",
        "\n",
        "ckpt = torch.load(\"/content/best_swa_model.pth\", map_location=\"cpu\")\n",
        "clean_sd = strip_module_prefixes(ckpt)\n",
        "\n",
        "model = MLP(...)\n",
        "model.load_state_dict(clean_sd, strict=True)  # strict=True si correspondance parfaite\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "C897VyuARnxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def calibrate_slice_norm(\n",
        "    mlp,\n",
        "    T_norm,          # tensor (N,1) ou scalaire déjà normalisé pour la slice\n",
        "    logm_norm,       # tensor (N,1) déjà normalisé\n",
        "    IV_norm_target,  # tensor (N,1) IV normalisée cible (même normalisation que train)\n",
        "    steps_adam=400, lr=1e-2, steps_lbfgs=100,\n",
        "    lambda_reg=0.0,  # petite régul L2 sur les 4 params normalisés\n",
        "    init_theta=None  # optionnel: tensor(4,) initialisation en espace \"norm\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Retourne theta_norm* de shape (4,) = [H_tilde_norm, eta_tilde_norm, rho_tilde_norm, sigma_tilde_norm]\n",
        "    qui minimise MSE( mlp([theta_norm, T_norm, logm_norm]), IV_norm_target ) sur la slice.\n",
        "    \"\"\"\n",
        "    device = next(mlp.parameters()).device\n",
        "    mlp.eval()\n",
        "    for p in mlp.parameters():\n",
        "        p.requires_grad_(False)\n",
        "\n",
        "    # Assure les shapes (N,1)\n",
        "    def as_col(x):\n",
        "        x = torch.as_tensor(x, dtype=torch.float32, device=device)\n",
        "        return x.view(-1,1)\n",
        "\n",
        "    Tn  = as_col(T_norm)\n",
        "    Kn  = as_col(logm_norm)\n",
        "    Yn  = as_col(IV_norm_target)\n",
        "\n",
        "    N = Tn.size(0)\n",
        "    assert Kn.size(0) == N and Yn.size(0) == N, \"T_norm, logm_norm, IV_norm_target doivent avoir la même longueur N\"\n",
        "\n",
        "\n",
        "    if init_theta is None:\n",
        "        theta_norm = torch.zeros(4, device=device, requires_grad=True)  # start à 0 en z-score\n",
        "    else:\n",
        "        theta_norm = torch.as_tensor(init_theta, dtype=torch.float32, device=device, requires_grad=True)\n",
        "\n",
        "    # Construit l'input (N,6) pour TON mlp: [theta_norm(4), T_norm, logm_norm]\n",
        "    def forward_loss():\n",
        "        theta_rep = theta_norm.unsqueeze(0).expand(N, -1)\n",
        "        X = torch.cat([theta_rep, Tn, Kn], dim=1)\n",
        "        IV_pred = mlp(X).view(-1,1)\n",
        "        loss = F.mse_loss(IV_pred, Yn)\n",
        "        if lambda_reg > 0:\n",
        "            loss = loss + lambda_reg * (theta_norm**2).mean()\n",
        "        return loss\n",
        "\n",
        "    # Optimisation pour sortir les paramètre finaux de notre surrogate : Adam + LBFGS\n",
        "    opt = torch.optim.Adam([theta_norm], lr=lr)\n",
        "    for _ in range(steps_adam):\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        L = forward_loss()\n",
        "        L.backward()\n",
        "        opt.step()\n",
        "\n",
        "    opt2 = torch.optim.LBFGS([theta_norm], max_iter=steps_lbfgs, line_search_fn=\"strong_wolfe\")\n",
        "    def closure():\n",
        "        opt2.zero_grad(set_to_none=True)\n",
        "        L = forward_loss()\n",
        "        L.backward()\n",
        "        return L\n",
        "    opt2.step(closure)\n",
        "\n",
        "    return theta_norm.detach()  # (4,) dans l'espace normalisé\n"
      ],
      "metadata": {
        "id": "bxg4hKntGPv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXCEL_PATH = \"/content/surafce de vol.xlsx\"   # 6 lignes x 19 colonnes d'IV brutes (sans header si rien)\n",
        "IV_IN_PERCENT = False                # mettre True si l'Excel est en %\n",
        "\n",
        "\n",
        "T_grid = np.sort(np.unique(np.round(df_train[\"T_norm\"].to_numpy(), 8)))[:6]\n",
        "K_grid = np.sort(np.unique(np.round(df_train[\"logm_norm\"].to_numpy(), 8)))[:19]\n",
        "\n",
        "print(\"T_norm grid (6):\", T_grid)\n",
        "print(\"logm_norm grid (19):\", K_grid)\n",
        "\n",
        "IV_raw = pd.read_excel(EXCEL_PATH, header=None).iloc[:6, :19].to_numpy(dtype=np.float32)\n",
        "if IV_IN_PERCENT:\n",
        "    IV_raw = IV_raw / 100.0  # si l'Excel est en %\n",
        "\n",
        "# construit le DataFrame long (114 lignes)\n",
        "T_col    = np.repeat(T_grid, len(K_grid))\n",
        "logm_col = np.tile(K_grid, len(T_grid))\n",
        "IV_col   = IV_raw.reshape(-1)   # flatten row-major: ligne par ligne\n",
        "\n",
        "df_market = pd.DataFrame({\n",
        "    \"T_norm\": T_col,\n",
        "    \"logm_norm\": logm_col,\n",
        "    \"IV\": IV_col\n",
        "})\n",
        "\n",
        "print(df_market.head(12))\n",
        "print(f\"df_market: {df_market.shape[0]} lignes (attendu 114)\")\n"
      ],
      "metadata": {
        "id": "4DJ4keWuTiEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_market.describe()"
      ],
      "metadata": {
        "id": "1ouvRCjNYn-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constantes de normalisation (On a normalisé et prétraité dans un autre code donc on les récup à la main)\n",
        "MU_IV  = 0.272141\n",
        "STD_IV = 0.115593\n",
        "\n",
        "df_market_norm = df_market.copy()\n",
        "df_market_norm[\"IV_norm\"] = ((df_market_norm[\"IV\"]/100) - MU_IV) / STD_IV\n",
        "\n",
        "print(df_market_norm.head(12))\n",
        "print(\n",
        "    \"Stats IV_norm -> mean≈\",\n",
        "    float(df_market_norm[\"IV_norm\"].mean()),\n",
        "    \" std≈\",\n",
        "    float(df_market_norm[\"IV_norm\"].std())\n",
        ")\n"
      ],
      "metadata": {
        "id": "mFmkyVcQYPC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_market_norm.describe()"
      ],
      "metadata": {
        "id": "r6D2OHK0Tk_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# La on applique notre modèle qui nous sort les 4 outputs\n",
        "T_all  = df_market_norm[\"T_norm\"].values.reshape(-1,1)\n",
        "K_all  = df_market_norm[\"logm_norm\"].values.reshape(-1,1)\n",
        "IV_all = df_market_norm[\"IV_norm\"].values.reshape(-1,1)\n",
        "\n",
        "theta_norm_star = calibrate_slice_norm(\n",
        "    mlp,\n",
        "    T_norm=T_all,\n",
        "    logm_norm=K_all,\n",
        "    IV_norm_target=IV_all,\n",
        "    steps_adam=400, lr=1e-2, steps_lbfgs=100,\n",
        "    lambda_reg=1e-4\n",
        ")\n",
        "\n",
        "print(\"θ* (global, espace normalisé) =\", theta_norm_star.cpu().numpy())\n"
      ],
      "metadata": {
        "id": "XMhDkbAXSlNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#On retire la noramlisaiton\n",
        "\n",
        "theta_norm = theta_norm_star.cpu().numpy()\n",
        "\n",
        "# Moyennes & std dans l'espace TILDE (issues du TRAIN)\n",
        "mu  = np.array([-2.148005,  0.692036, -0.746259, -1.453146])\n",
        "std = np.array([ 0.556125,  0.400578,  0.306652,  0.379126])\n",
        "\n",
        "theta_tilde = theta_norm * std + mu\n",
        "\n",
        "H_tilde, eta_tilde, rho_tilde, sigma_tilde = theta_tilde\n",
        "print(\"Tilde-space:\")\n",
        "print(\"H̃ =\", H_tilde, \"| η̃ =\", eta_tilde, \"| ρ̃ =\", rho_tilde, \"| σ̃ =\", sigma_tilde)\n"
      ],
      "metadata": {
        "id": "X4PtBlALbxZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#On retire les Tilde (filtre qu'on a utilisé pour ouvrir nos paramètres sur R)\n",
        "def sigmoid(x): return 1/(1+np.exp(-x))\n",
        "\n",
        "H     = sigmoid(H_tilde)\n",
        "eta   = np.exp(eta_tilde)\n",
        "rho   = np.tanh(rho_tilde)\n",
        "sigma = np.exp(sigma_tilde)\n",
        "\n",
        "print(\"\\nParamètres physiques SVI retrouvés :\")\n",
        "print(f\"H     = {H:.6f}\")\n",
        "print(f\"eta   = {eta:.6f}\")\n",
        "print(f\"rho   = {rho:.6f}\")\n",
        "print(f\"sigma = {sigma:.6f}\")\n",
        "\n",
        "#Ici nous avons enfin retrouver nos 4 paramètre du modèle roughBergomi"
      ],
      "metadata": {
        "id": "tSOTxqs1b0rj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}